---
title: "Leveraging Intermediate Neural Collapse with Simplex ETFs for Efficient Deep Neural Networks"
collection: papers
# category: manuscripts
permalink: /papers/2024-12-14-nc-etfs
excerpt: 'This paper examines the phenomenon of neural collapse, where network activations, class means, and linear classifier weights converge to a simplex equiangular tight frame (ETF) during training, enhancing interpretability, robustness, and generalization. Building on findings that neural collapse extends beyond the final layer in fully connected networks, the authors propose two novel methods: Adaptive-ETF, which enforces simplex ETF constraints across all layers beyond a certain depth, and ETF-Transformer, which applies these constraints to feedforward layers in transformer blocks. Both methods maintain performance while significantly reducing trainable parameters, offering efficient alternatives for network training and regularization.'
date: 2024-12-14
venue: 'NeuRIPS 2024 Workshop on Mathematics of Modern Machine Learning'
# slidesurl: 'http://academicpages.github.io/files/slides2.pdf'
# paperurl: 'http://academicpages.github.io/files/paper2.pdf'
citation: 'Leveraging Intermediate Neural Collapse with Simplex ETFs for Efficient Deep Neural Networks (2024). Emily Liu. NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning.'
---
  Neural collapse is a phenomenon observed during the terminal phase of neural network training, characterized by the convergence of network activations, class means, and linear classifier weights to a simplex equiangular tight frame (ETF), a configuration of vectors that maximizes mutual distance within a subspace. This phenomenon has been linked to improved interpretability, robustness, and generalization in neural networks. However, its potential to guide neural network training and regularization remains underexplored. Previous research has demonstrated that constraining the final layer of a neural network to a simplex ETF can reduce the number of trainable parameters without sacrificing model accuracy. Furthermore, deep fully connected networks exhibit neural collapse not only in the final layer but across all layers beyond a specific effective depth. Using these insights, we propose two novel training approaches: **Adaptive-ETF**, a generalized framework that enforces simplex ETF constraints on all layers beyond the effective depth, and **ETF-Transformer**, which applies simplex ETF constraints to the feedforward layers within transformer blocks. We show that these approaches achieve training and testing performance comparable to those of their baseline counterparts while significantly reducing the number of learnable parameters.
